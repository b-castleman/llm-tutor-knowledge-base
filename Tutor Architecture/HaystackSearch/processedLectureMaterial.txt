	Topic Name: Gestation of AI.	Topic Keywords: [gestation of ai, artificial neuron, mcculloch and pitts, biological neuron, nerve cell, dendrites, cell body, axon, binary inputs, threshold value, propositional logic, alan turing, turing test, language, knowledge, learning, understanding, ai agenda, ].	Topic Information: Topic titled: Gestation of AI Era: First the Gestation of AI. In the early 1940s,  we started seeing the early signs of AI, the most prominent ones started with seminal work by McCulloch and Pitts. They showed that we can mimic the activity of a biological neuron with an artificial neuron.  A biological neuron is a nerve cell representing the functional unit in the brain  which contains an average of 86 billion neurons, connected to one another.  Each neuron has the ability to receive and transmit nerve impulses. <could be electrical or chemical> A neuron is composed of three parts: dendrites to receive impulses from other neurons,  a cell body to process them and an axon to transmit the information to other neurons. An artificial  neuron is similar and mimics this functionality. It can take a set of binary inputs that the neuron body  aggregates by adding them up. If the sum exceeds a threshold value that can be adjusted, then the neuron fires by generating an output of 1. If the sum does not exceed the threshold, then the neuron outputs a 0.  The neuron can learn a propositional logic expression, that is an expression that combines statements with connectives such as “and”, “or”, etc. In 1950, Alan Turing published his paper “Computing Machinery and Intelligence”.  Besides proposing the Turing test, he laid the ground for AI major concepts  including language, knowledge, learning, and understanding; he had a clear vision  for a full AI agenda. 
	Topic Name: Early Enthusiasm and Expectations.	Topic Keywords: ["dartmouth meeting", "ai research", "summer research project on ai", "john mccarthy", "marvin minsky", "nathaniel rochester", "claude shannon", "thinking machines", "mathematics proving system", "newell and simon", "checkers playing program", "reinforcement learning", "arthur samuel", "ibm", "average player", "robert nealey", "game theory", "ibm deep blue", "deepmind’s alphago", "lisp", "high-level programming language", "ai research", "symbolic ai", "knowledge representation", "reasoning", "minsky’s microworlds", "microworlds", "algebra", "integrals", "geometry", "shrdlu", "natural understanding program", "terry winograd", "mit", "logical reasoning", "deduction", "neural network", "bernie widrow", "adaline", "frank rosenblatt", "perceptron", "mcculloch and pitts' neuron", "weighted sum", "adjustable threshold", "shakey", "ai researchers", "artificial intelligence center", "stanford research institute", "mobile intelligent robot", "logical reasoning", "sensing", "navigational techniques", "planning", "execution", "self-driving cars", "robots", "mars rovers", "computer history museum", ].	Topic Information: Topic titled: Early Enthusiasm and Expectations Era: %% Dartmouth meeting (part 7.3)  1956 was marked by a seminal event in the history of AI: the summer research project on AI took place at Dartmouth college in Hanover, New Hampshire. The gathering was proposed by John McCarthy,  joined by Marvin Minsky, Nathaniel Rochester and Claude Shannon. The event gathered about 20 participants to discuss ideas about the “thinking machines”. It is largely recognized as the event that kicked off the field of Artificial intelligence %A highlight of this event was a mathematics proving system by Newell and Simon.   %% Samuel’s checkers (part 7.3)  In 1956, Arthur Samuel, a pioneer in game theory, developed at IBM a checkers playing program using reinforcement learning, a way to teach the machine to self-learn. <insist on the world self-learn, may be say it slow)>  His program was able to outperform an average player, and was even shown to the public on TV playing against master player Robert Nealey. Samuel’s checkers marked an important milestone in AI and game theory and constituted the precursor of today’s famous AIs like IBM Deep Blue or Deepmind’s AlphaGo.   %% Lisp (part 7.4)  In 1958, McCarthy invented Lisp, a high-level programming language used in AI research that dominated the field for 30 years <SLOW on 30 years>.  He also wrote the article “programs with common sense” that laid the ground to symbolic AI, knowledge representation and reasoning.   %% Minsky’s microworlds (part 7.5)  Between 1963 and early 1970, Minsky’s students developed what is  known as Microworlds <slow on microworlds> to solve problems in algebra, integrals and geometry..   %% SHRDLU  (part 7.6)  The most famous application of Blockworlds was SHRDLU <pronounce sure’d’lu> , a   natural understanding program  developed by Terry Winograd at MIT between 1968 and 1970.  SHRDLU is an AI that can understand and discuss some subjects in English. It can 	manipulate toy objects, and respond by taking actions or asking for clarification.  The SHRDLU program was written in Lisp and integrates syntax, semantics, inference that makes it capable of logical reasoning, and deduction.   %% NN (part 7.7)  The early 60s marked the development of the first neural network, which is a network of neurons.  Bernie Widrow and his student invented Adaline. Meanwhile, Frank Rosenblatt invented the  Perceptron.  Keep in mind, they were both built upon McCulloch and Pitts’ neuron, but this time, the inputs can be real-valued rather than only binary, and there are weights associated with them to adjust their importance. Just like McCulloch and Pitts neuron, the inputs are aggregated but this time with a weighted sum and the neuron fires accordingly based on the neuron adjustable threshold.               %% Shakey (part 7.8)  Between 1966-1972 Shakey the robot  was invented by a team of AI researchers at  The Artificial Intelligence Center at Stanford Research Institute. %at the Artificial Intelligence Center at Stanford Research Institute.  It is the World’s first mobile intelligent robot that could sense the world, and use logical reasoning to make decisions. Shakey could sense, learn, use early navigational techniques to plan, carry out plans of action and communicate in simple English.   %It had a camera, an antenna, a bump detector, and a push bar to move objects around.  The robot was named Shakey because it shook a lot when it moved, as stated by one of its inventors, and hence its name! Its hardware was not very sophisticated, however, its AI system was quite advanced, and considered as an important milestone in AI history, as it gathered many AI capabilities, including sensing, planning, language and execution in the same AI.  It is recognized as the precursor of self-driving cars, today’s robots and even Mars rovers. Today, Shakey is in the computer history museum in California.  
	Topic Name: Knowledge-Based AI.	Topic Keywords: [knowledge-based ai, 1960s, interest in ai, wane, early ai systems, toy examples, scale, large problems, complex problems, perceptron, simple, linear problems, funding, ai research, reduce, domain knowledge, encode information, world, expert systems, human decision making processes, rules, dendral, chemistry, mycin, medicine, industry, 80s, build, maintain, uncertainty, learn from experience, ai winter, ].	Topic Information: Topic titled: Knowledge-Based AI Era: %% 1960s: interest in AI begins to wane. Unfortunately, the interest in AI began to wane in the 60’s, mainly because early AI systems were developed on toy examples and therefore did not scale to large and complex problems. For instance, the perceptron was deemed too simple as it was limited to linear problems. Consequently, funding for AI research was reduced significantly <slow on significantly>.   %% Idea of using domain knowledge (part 7.9)  Then an idea emerged! <may be add an idea sound cling> How about incorporating domain knowledge to encode information about the world? This idea led to the creation of what is known as expert systems that can simulate human decision making processes using rules. Expert systems like Dendral in chemistry or Mycin in medicine and others were successfully developed and turned AI to an industry in the 80s.   %% Expert systems (part 7.10)  However, expert systems were limited because they were hard to build and maintain as  they could not handle uncertainty about the world nor learn from experience.   Consequently, AI research slowed <slow on slowed> down in a period known as the AI winter… 
	Topic Name: AI Becomes Scientific.	Topic Keywords: [ai becomes scientific, expert systems, neural networks, symbolic logic, connectionist approaches, backpropagation algorithm, feedforward, probability, boolean logic, machine learning, benchmark data, big data, world wide web, ibm watson, jeopardy, deep learning, convolutional neural networks, ai spring, ].	Topic Information: Topic titled: AI Becomes Scientific Era: %% AI becomes scientific  (part 7.11)   The failure of expert systems was an incentive to the development of a more successful, scientific and unified approach to AI from the 1990s to the present. Many subfields came together to make AI a success! %% 1986 the return of neural networks (part 7.11) First, the community turned to connectionist approaches, that is using neural networks instead of symbolic logic. Furthermore, many neurons could be put together in a network and learn using the backpropagation algorithm <slow on the backpropagation algorithm>: Inputs are fed into the neural network and then the errors are back propagated to help fix the parameters.   Neural networks are better suited to address the “imperfections” of the world. Also, the process goes on with feedforward and backpropagation as the network learns from  input data and adapts its parameters to a variety of imperfect inputs.   %%Probability (part 7.12)  The failure of expert systems also led to the incorporation of PROBABILITY rather than Boolean logic. Why?  While Boolean logic is rigorous and powerful to model certainty, we are left with a large number of problems that require modeling uncertainty.  The theory of probability provides a rigorous framework to reason about uncertainty, quantify it, and study the laws that govern chance. Probability allowed AI to make a huge leap by modeling uncertainty in applications as diverse as machine translation and robotics, to cite a few.   %% Machine learning (part 7.13)  Another important ingredient to the success of AI is using machine learning <slow on machine  learning>, that is learning from experience, from data,  rather than hard coding that can be tedious and fragile.   %% Benchmark data (part 7.14)  Developing machine learning was facilitated by the availability of benchmark datasets and data repositories that helped researchers in the design and testing of their algorithms. Examples include the MNIST dataset for digits, or the imagenet repository for images. This helped overcome the limits and availability of toy examples that AI was using earlier.   %% Big data (part 7.15)  Furthermore, collecting and using data for machine learning was facilitated by the World Wide  Web that made big data <slow on Big Data> available in different forms, like text, images,  audio, video, genomic and  social networks data.   With big data and ML, and AI success in vision, games, machine translation, and speech  recognition,  AI finally regained its lost commercial and public interest.  This success was empowered by events like the participation of the  IBM WATSON AI in the TV  quiz show Jeopardy.  Watson demonstrated advanced AI capabilities including speech recognition, search, Natural  Language Processing, Information extraction, building answers, converting them from  text into speech, and interacting with the game. Watson's avatar was sitting behind a podium  and even hitting the buzz button like the other human contestants.  IBM Watson, outplayed two human Jeopardy! Champions. This was a great moment and  boost for AI!   %% Deep learning (part 7.16)  The last but not the least ingredient to AI succes is deep learning <slow on deep learning>, that  is using a type of neural networks that is called “deep” because it is made of many layers of  neurons.  One example is the use of Convolutional Neural Networks <slow>  (CNN). While they were  invented in the 1970’s, it was only starting 2011 that they could be used at their full capacity. This is thanks to the use of powerful hardware like FPGA, GPUs, and  TPUs., that can provide  the computational power needed to handle many layers with billions of parameters to find.   %% AI spring (part 7.17)  This along with machine learning advanced algorithms trained on big data brought AI to the next  level and to what is known as the AI Spring, where AI is a flourishing multidisciplinary field.  
